# ML Prediction Service

Production-ready microservice for equipment failure prediction using trained Random Forest models. Provides REST API endpoints for real-time failure predictions based on sensor readings.

## ðŸŽ¯ Service Overview

**Domain**: Machine Learning Inference  
**Port**: 8002  
**Version**: 1.0.0

### Responsibilities

- Load trained ML models (Random Forest classifier)
- Provide real-time failure predictions via REST API
- Calculate failure probability and severity levels
- Support single and batch predictions
- Expose model metadata and performance metrics

## ðŸ“‹ Features

âœ… **REST API** with FastAPI and OpenAPI documentation  
âœ… **Model Loading** - Loads trained Random Forest, scaler, and metadata  
âœ… **Real-time Predictions** - Fast inference (<100ms response time)  
âœ… **Batch Processing** - Process up to 100 predictions per request  
âœ… **Severity Classification** - CRITICAL, HIGH, MEDIUM, LOW  
âœ… **Confidence Scores** - High, medium, low confidence levels  
âœ… **Health Checks** - Kubernetes-ready liveness/readiness probes  
âœ… **Structured Logging** - JSON logs to stdout  
âœ… **Docker Ready** - Multi-stage build with non-root user  

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP POST
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML Prediction API      â”‚
â”‚     (FastAPI)           â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
    â”‚              â”‚
    â”‚ Load         â”‚ Predict
    â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ML Models   â”‚  â”‚  Sensor Data â”‚
â”‚(*.pkl,json)â”‚  â”‚  (validated) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Trained ML model files (from `scripts/train_model.py`)
- Docker (optional)

### 1. Setup Model Files

First, place your trained model files in the `models/` directory:

```bash
services/ml-prediction/models/
â”œâ”€â”€ failure_predictor_v1.pkl
â”œâ”€â”€ scaler_v1.pkl
â”œâ”€â”€ feature_names_v1.json
â””â”€â”€ model_metadata_v1.json
```

These files are generated by running:
```bash
python scripts/train_model.py \
  --data-path ./data/processed/train_data.csv \
  --output-dir ./services/ml-prediction/models
```

### 2. Local Development (without Docker)

```bash
# Navigate to service directory
cd services/ml-prediction

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment variables
cp .env.example .env

# Run the service
python -m uvicorn app.main:app --reload --port 8002
```

### 3. Docker

```bash
# Build image
docker build -t ml-prediction:latest .

# Run container
docker run -p 8002:8002 \
  -v $(pwd)/models:/app/models \
  ml-prediction:latest
```

### 4. Verify Installation

```bash
# Check health
curl http://localhost:8002/health

# View API documentation
open http://localhost:8002/docs
```

## ðŸ“¡ API Endpoints

### Prediction Endpoints

#### POST `/api/v1/predict` - Single Prediction

Make a failure prediction for equipment based on sensor readings.

**Request:**
```json
{
  "equipment_id": "RADAR-LOC-001",
  "temperature": 85.5,
  "vibration": 0.45,
  "pressure": 3.2,
  "humidity": 65.0,
  "voltage": 220.0
}
```

**Response (200 OK):**
```json
{
  "equipment_id": "RADAR-LOC-001",
  "prediction": 1,
  "failure_probability": 0.8523,
  "severity": "HIGH",
  "days_until_failure": 15,
  "confidence": "high",
  "timestamp": "2025-11-02T10:30:00Z",
  "model_version": "v1"
}
```

**cURL Example:**
```bash
curl -X POST http://localhost:8002/api/v1/predict \
  -H "Content-Type: application/json" \
  -d '{
    "equipment_id": "RADAR-LOC-001",
    "temperature": 85.5,
    "vibration": 0.45,
    "pressure": 3.2,
    "humidity": 65.0,
    "voltage": 220.0
  }'
```

#### POST `/api/v1/predict/batch` - Batch Predictions

Process multiple predictions in a single request (max 100).

**Request:**
```json
{
  "readings": [
    {
      "equipment_id": "RADAR-LOC-001",
      "temperature": 85.5,
      "vibration": 0.45,
      "pressure": 3.2
    },
    {
      "equipment_id": "RADAR-LOC-002",
      "temperature": 78.2,
      "vibration": 0.38,
      "pressure": 3.0
    }
  ]
}
```

**Response (200 OK):**
```json
{
  "predictions": [
    {
      "equipment_id": "RADAR-LOC-001",
      "prediction": 1,
      "failure_probability": 0.8523,
      "severity": "HIGH",
      ...
    },
    {
      "equipment_id": "RADAR-LOC-002",
      "prediction": 0,
      "failure_probability": 0.2156,
      "severity": "LOW",
      ...
    }
  ],
  "total": 2
}
```

### Model Information

#### GET `/api/v1/model/info` - Model Metadata

Get information about the loaded ML model.

**Response (200 OK):**
```json
{
  "model_type": "RandomForestClassifier",
  "version": "v1",
  "trained_date": "2025-11-02T10:30:00Z",
  "features": ["temperature", "vibration", "pressure", "humidity", "voltage"],
  "accuracy": 0.8810,
  "precision": 0.8523,
  "recall": 0.9095,
  "f1_score": 0.8800,
  "roc_auc": 0.9345
}
```

### Health Checks

#### GET `/health` - Health Check

Kubernetes liveness probe endpoint.

**Response (200 OK):**
```json
{
  "service": "ml-prediction-service",
  "status": "healthy",
  "version": "1.0.0",
  "model_loaded": true,
  "model_version": "v1",
  "timestamp": "2025-11-02T10:30:00Z"
}
```

## ðŸ”§ Configuration

All configuration via environment variables (see `.env.example`):

### Required Variables

```bash
# Model paths (REQUIRED)
MODEL_PATH=./models/failure_predictor_v1.pkl
SCALER_PATH=./models/scaler_v1.pkl
FEATURE_NAMES_PATH=./models/feature_names_v1.json
```

### Optional Variables

```bash
# Service
SERVICE_NAME=ml-prediction-service
PORT=8002
LOG_LEVEL=INFO

# Prediction
FAILURE_THRESHOLD=0.5

# API
API_V1_PREFIX=/api/v1
```

## ðŸ“Š Prediction Interpretation

### Severity Levels

| Probability Range | Severity | Days Until Failure | Action Required |
|-------------------|----------|-------------------|-----------------|
| â‰¥ 80% | CRITICAL | 7 days | Immediate maintenance |
| 60-79% | HIGH | 15 days | Schedule maintenance soon |
| 40-59% | MEDIUM | 30 days | Plan maintenance |
| < 40% | LOW | 60 days | Monitor |

### Confidence Levels

- **High** - Probability very close to 0 or 1 (clear decision)
- **Medium** - Moderate certainty
- **Low** - Probability near 0.5 (uncertain, monitor closely)

## ðŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=app --cov-report=html

# Run specific test file
pytest tests/test_main.py -v
```

## ðŸ³ Docker

### Build

```bash
docker build -t ml-prediction:latest .
```

### Run

```bash
docker run -p 8002:8002 \
  -v $(pwd)/models:/app/models \
  -e LOG_LEVEL=INFO \
  ml-prediction:latest
```

### Docker Compose

Add to your `docker-compose.yml`:

```yaml
services:
  ml-prediction:
    build: ./services/ml-prediction
    ports:
      - "8002:8002"
    volumes:
      - ./services/ml-prediction/models:/app/models
    environment:
      - MODEL_PATH=/app/models/failure_predictor_v1.pkl
      - SCALER_PATH=/app/models/scaler_v1.pkl
      - FEATURE_NAMES_PATH=/app/models/feature_names_v1.json
    restart: unless-stopped
```

## ðŸš¢ Deployment

### Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-prediction
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: ml-prediction
        image: ml-prediction:latest
        ports:
        - containerPort: 8002
        volumeMounts:
        - name: models
          mountPath: /app/models
          readOnly: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8002
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8002
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: ml-models-pvc
```

## ðŸ“ˆ Performance

- **Response Time**: <100ms per prediction
- **Throughput**: ~1000 predictions/second (single worker)
- **Memory Usage**: ~200MB (with model loaded)
- **Model Loading**: ~2-5 seconds at startup

## ðŸ”— Integration

### Python Client Example

```python
import requests

# Make prediction
response = requests.post(
    "http://localhost:8002/api/v1/predict",
    json={
        "equipment_id": "RADAR-LOC-001",
        "temperature": 85.5,
        "vibration": 0.45,
        "pressure": 3.2,
        "humidity": 65.0,
        "voltage": 220.0
    }
)

prediction = response.json()

if prediction["prediction"] == 1:
    print(f"âš ï¸ Failure likely in {prediction['days_until_failure']} days")
    print(f"Severity: {prediction['severity']}")
    print(f"Probability: {prediction['failure_probability']:.2%}")
else:
    print("âœ“ Equipment operating normally")
```

### JavaScript Client Example

```javascript
const response = await fetch('http://localhost:8002/api/v1/predict', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    equipment_id: 'RADAR-LOC-001',
    temperature: 85.5,
    vibration: 0.45,
    pressure: 3.2,
    humidity: 65.0,
    voltage: 220.0
  })
});

const prediction = await response.json();
console.log('Prediction:', prediction);
```

## ðŸ› Troubleshooting

### Error: Model files not found

```
FileNotFoundError: Required model files not found
```

**Solution**: Ensure model files are in the `models/` directory. Train a model first:
```bash
python scripts/train_model.py \
  --data-path ./data/processed/train_data.csv \
  --output-dir ./services/ml-prediction/models
```

### Error: Model not loaded

```
503 Service Unavailable: ML model not loaded
```

**Solution**: Check logs for model loading errors. Verify model file paths in `.env`.

### High prediction latency

**Solution**: 
- Increase number of workers: `--workers 4`
- Use production ASGI server settings
- Enable caching if needed

## ðŸ“š Project Structure

```
services/ml-prediction/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ models.py            # Pydantic models
â”‚   â”œâ”€â”€ ml_model.py          # ML model loader
â”‚   â”œâ”€â”€ services.py          # Business logic
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â””â”€â”€ utils.py             # Utilities
â”œâ”€â”€ models/
â”‚   â””â”€â”€ .gitkeep            # Model files directory
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_main.py        # API tests
â”‚   â””â”€â”€ test_ml_model.py    # Model tests
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ðŸ“ž Support

For issues or questions:
1. Check logs for error messages
2. Verify model files are present and valid
3. Ensure all dependencies are installed
4. Review API documentation at `/docs`

---

**Service Status**: âœ… Production Ready  
**Last Updated**: 2025-11-02  
**Maintainer**: DRDO Summer Training Project
